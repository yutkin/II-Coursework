\section*{Основные термины, определения и сокращения}
\textbf{API (Application Programming Interface)} "--- набор функций, процедур, структур и констант, предоставляемых сервисом или 
библиотекой стороннему разработчику. 

\textbf{BLAS (Basic Linear Algebra Subprograms)} "--- API для создания приложений, выполняющих базовые операции линейной алгебры.

\textbf{Batch normalization} "--- метод, используемый для нормализации данных <<протекающих>> в нейронной сети в процессе её обучения.

\textbf{CLI (Command Line Interface)} --- тип интерфейса, в котором инструкции передаются компьютеру путём ввода с клавиатуры.

\textbf{СНН (Свёрточные нейронные сети)} "--- специализированный тип нейронных сетей,
который служит для работы с данными, имеющими пространственную топологию. Например, временные ряды или изображения.
Нейросеть называется свёрточной, если использует хотя бы в одном из своих слоёв математическую операцию свёртки.

\textbf{cuBLAS} "--- реализация BLAS для NVIDIA GPUs.

\textbf{cuDNN} "--- библиотека для работы с примитивами глубоких нейронных сетей (свёртка, pooling, нормализация,
активации и тд.) на GPUs.

\textbf{L2 regularization} "--- один из способов регуляризации. Реализуется путём добавления слагаемого $\frac{1}{2}\lambda W^2$ в 
шаге SGD, где $\lambda$ "--- коэффициент регуляризации, $W$ "--- параметры модели.

\textbf{LMDB (Lightning Memory-Mapped Database)} "--- программное обеспечение, предоставляющее высокопроизводительные базы данных,
с упорядоченным хранилищем вида ключ-значение.

\textbf{ReLu (Rectified Linear Unit)} "--- функция активации вида $f(x) = \max(0, x)$.

\textbf{Leaky ReLU} "--- модификация ReLU. Имеет вид  $f(x)=\alpha x$, если $x<0$ и $f(x)=x$, если $x \geq 0$.

\textbf{Maxout unit} "--- функция активации вида $h_i(x) = \max_{j \in [1,k]}(x^T W_{ij} + b_{ij})$. Данная функция вычисляет
максимум из $k$ своих входов.

\textbf{Mini batch} "--- небольшая группа входных примеров (в свёрточных нейронных сетях --- изображения), которая используется
при вычислении ошибки в одной итерации SGD.

\textbf{LR (learning rate)} "--- задаёт размер шага в алгоритме градиентного спуска. Влияет на скорость обучения нейронной сети.

\textbf{SGD (Stochastic gradient descent with momentum)} "--- стохастическая аппроксимация градиентного спуска,
в которой ошибка вычисляется не на всём обучающем множестве, а на каком то случайном его подмножестве, называемое mini batch.
Шаг обновления весов выглядит следующим образом:
\begin{align*}
    v_{t+1} &= \mu v_{t} - \gamma \nabla E(z_i, w_t),\\
    w_{t+1} &= w_t + v_t,
\end{align*}

где $w_i$ "--- веса модели на $i$-ой итерации, $\gamma$ "--- learning rate, $E$ "--- функционал ошибки,
$z_i$ "--- mini batch, $t$ "--- номер итерации, $v_i$ "--- момент.

\textbf{NAG (Nesterov accelerated gradient)} "--- модификация алгоритма стохастического градиентного спуска. В отличии от SGD, в NAG
формула для обновления весов имеет следующий вид:
\begin{align*}
    v_{t+1} &= \mu v_{t} - \gamma \nabla E(z_i, w_t + \mu v_t),\\
    w_{t+1} &= w_t + v_t,
\end{align*}
где $w_i$ "--- веса модели на $i$-ой итерации, $\gamma$ "--- learning rate, $E$ "--- функционал ошибки,
$z_i$ "--- mini batch, $t$ "--- номер итерации, $v_i$ "--- момент.

\textbf{Pooling} "--- один из слоёв свёрточных нейронных сетей.
Выполняет уменьшение размерностей активаций предыдущего свёрточного слоя.

\textbf{Stacking} "--- один из способов построения ансамбля. Идея <<стэкинга>> заключается в обучении
алгоритма, делающего финальные предсказания, на ответах базовых моделей.

\textbf{Stride} "--- один из параметров свёрточного слоя. Отвечает за размер шага, с которым ядро свёртки <<перемещается>>
вдоль входного изображения.

\textbf{ZCA Whitening (Zero-phase Component Analysis Whitening)} "--- алгоритм препроцессинга данных. Если входные данные
являются изображениями, то данный метод избавляет их от лишней информации, уменьшает взаимную корреляцию и добивается
их одинаковой дисперсии.