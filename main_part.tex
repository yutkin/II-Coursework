\section{Основная часть}
\subsection{Оборудование, использовавшиеся в работе}
Традиционно, нейронные сети обучались с использованием CPU на одной машине. Сегодня такой подход
является неэффективным. Современные библиотеки для работы с глубокими нейронными сетями
используют GPU или распределённые CPU вычисления между несколькими машинами.
Свёрточные сети зачастую хранят огромное число буферов параметров, активаций и градиентов.
Все эти значения должны быть обработаны в течении каждого шага обучения. Количество таких буферов
может быть настолько велико, что может не помещаться в кэш традиционных десктопных компьютеров.
Напротив, пропускная способность памяти графических процессоров намного выше, чем в CPU.
Кроме того, алгоритмы нейронных сетей зачастую не требуют сложных ветвлений или рекурсивных вызовов,
что позволяет легко исполнять их на GPU. В силу того, что нейронные сети могут быть разделены на множество
индивидуальных нейронов, которые способны проводить вычисления независимо от других нейронов в слое,
нейронные сети получают большую выгоду от использования GPU параллелизма. \cite[стр. 448]{Goodfellow-et-al-2016-Book}
Таким образом, использование специализированного оборудования является важным фактором успешного исследования.

В данной работе эксперименты выполнялись с использованием веб-сервиса Amazon Elastic Compute Cloud, который
<<предоставляет масштабируемые вычислительные ресурсы в облаке и упрощает процесс крупномасштабных вычислений
для разработчиков>>\footnote{\url{https://aws.amazon.com/ec2/}}. Сервер, на котором обучались нейронные сети (g2.2xlarge),
имеет следующие характеристики:
\begin{itemize}
    \item CPU Intel Xeon E5-2670 2.6 GHz, 8 ядер;
    \item GPU NVIDIA GRID K520 4\,Gb, 1536 CUDA ядер;
    \item 16\,Gb RAM.
\end{itemize}

\subsection{Датасет CIFAR-10}
Набор данных, на котором проводились исследования, состоит из 60\,000 цветных изображений, размера 
32$\times$32 пикселя. Каждое изображение принадлежит одному из 10 классов\footnote{самолёт, автомобиль, птица,
кот, олень, собака, лягушка, лошадь, корабль, грузовик}, что соответствует 6\,000 изображениям на класс. Под 
обучение отводится 50\,000 изображений. Остальные 10\,000 используются для тестирования. Объекты в классах сильно варьируются, 
например, класс <<птица>> содержит различные виды птиц, как большие так и маленькие. Кроме того, объекты классов представлены в 
различных позах и под различными углами. Особенно это проявляется среди собак и котов, которые изображены не только в различных 
позах, но иногда и частично, например, изображена только голова животного.

Датасет CIFAR-10 \cite{learningmultiple} был выбран для исследований благодаря своему относительно небольшому размеру, 
который позволил обучать глубокие свёрточные нейронные сети, быстро проводить эксперименты и проверять на практике различные гипотезы.

На момент написания работы лучший результат (state-of-the-art) на CIFAR-10 95,53\% \cite{2014arXiv1412}. Точность  
распознавания человека $\approx$\,94\%.\footnote{\url{http://karpathy.github.io/2011/04/27/manually-classifying-cifar10/}}

\begin{figure}[H]
\centering
\includegraphics[width=0.5\textwidth]{cifar10}
\caption{10 случайных изображений из каждого класса}
\end{figure}

\subsection{Фреймворк Caffe}
Свёрточные нейронные сети обучались с помощью фреймворка для глубокого обучения Caffe \cite{jia2014caffe}.
Изначально Caffe разрабатывался командой BVLC (Berkeley Vision and Learning Center), но постепенно перерос в большой 
open-source проект.\footnote{\url{https://github.com/BVLC/caffe}} На данный момент вклад в развитие Caffe внесли почти 200 
разработчиков и более 10\,000 человек оценили проект на Github.

Данный фреймворк был выбран для решения задачи по нескольким причинам:
\begin{enumerate}
    \item Простота определения моделей и методов оптимизации. Архитектуры нейронных сетей
    (Caffe поддерживает топологии сетей в форме любых ациклических графов) и алгоритмы оптимизации удобно и 
    эффективно определяются в специальных конфигурационных файлах типа Google Protocol 
    Buffers\footnote{\url{https://developers.google.com/protocol-buffers/}}. 
    \item Модульность. Caffe позволяет легко изменять архитектуру сети под новые форматы входных данных. Кроме того, в наличии 
    имеется много слоёв и функций потерь.
    \item Скорость вычислений и эффективное использование ресурсов. Caffe заранее выделяет ровно столько памяти сколько нужно для 
    нейронной сети. Операций линейной алгебры такие как умножение, сложение и свёртка выполняются на CPU с помощью BLAS (Basic 
    Linear Algebra Subroutines). На GPU за эти операции отвечают библиотеки cuBLAS и cuDNN 
    \cite{DBLP:journals/corr/ChetlurWVCTCS14}
    \item CLI и интерфейсы для Python и Matlab.
\end{enumerate}

Caffe написан на языках C++, Cuda, Python. Для хранения датасетов используются эффективные базы данных 
LMDB\footnote{\url{http://symas.com/mdb/}} и LevelDB\footnote{\url{https://github.com/google/leveldb}}. Обучение нейронных сетей 
может выполняться как на CPU, так и на нескольких GPU одновременно. Фреймворк доступен для Linux, Windows и OS X.

\subsection{Предварительная обработка данных}
Необработанные изображения содержат излишнюю информацию, так как смежные пиксели имеют высокую корреляцию. Поэтому прежде чем 
подавать изображения на вход нейронной сети, они были обработаны в два этапа. В начале была произведена глобальная нормализация контраста
(global contrast normalization):
\[ \widehat{X} = \cfrac{X - \overline{X}}{\sigma},\]
где $X$ --- исходное изображение, $\overline{X}$ --- среднее значение, $\sigma$ --- стандартное отклонение.

Затем изображения были линейно трансформированы с помощью ZCA whitening \cite{learningmultiple}.
Цель данного алгоритма сделать входные изображения слабо коррелирующими друг с другом.

\begin{lstlisting}[language=Python, frame=TB, caption=Алгоритм ZCA whitening]
cov = np.dot(X.T, X) / X.shape[0] #%* Вычисляем ковариационную матрицу *)
U,S,V = np.linalg.svd(cov) #%* Находим сигнулярное разложение ковариацонной матрицы *)
Xrot = np.dot(X, U) #%* Поворачиваем входные данные *)
Xwhite = Xrot / np.sqrt(S + %*$\alpha$*)) #%* Делим на собственные числа *)
\end{lstlisting}
\vspace*{-0.4cm}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{cifar10zca}
    \caption{Изображения после применения ZCA whitening с параметром $\alpha=0,1$}
\end{figure}
Предобработка с помощью ZCA whitening уменьшила ошибку в среднем на $1,5\%$. Кроме того, каждое изображение было
увеличено до 40$\times$40 пикселей, для того чтобы в процессе обучения вырезать из 
изображения случайный патч размером 32$\times$32. Такой приём позволяет снизить эффект переобучения модели и повысить конечную 
точность распознавания.

\subsection{Обучение нейронных сетей}
Во время исследования были проверены различные гипотезы, архитектуры и эвристики так или иначе влияющие на конечные точности 
моделей. В результате различных экспериментов были отобраны семь нейронных сетей, показавшие наилучший результат.
В приложении отражены топологии сетей (таблица~\ref{models-table}), графики обучения (рисунок~\ref{fig:train_all}) и время,
затраченное на обучение каждой из моделей (рисунок~\ref{time_consumption}).

\subsubsection{Инициализация параметров}
Оптимизационные алгоритмы для глубоких нейронных сетей являются итеративными и, таким образом, требуют
от пользователя задания начальной точки, с которой нужно начать обучение. От выбора этой точки определятся
конечная сходимость алгоритма. Более того, выбор начальной точки определяет с какой скоростью
будет проходить обучение и какова будет ошибка в конечной точке, к которой сойдётся оптимизационный алгоритм.

Современные стратегии инициализации просты и основаны на различных эвристиках. Проектирование более сложных методов
является трудной задачей, так как оптимизация нейронных сетей ещё не достаточно изучена. В данной работе инициализация
проводилась с помощью layer-sequential unit-variance~\cite{DBLP:journals/corr/MishkinM15}.
Данный метод сначала проводит преинициализацию случайными ортонормированными матрицами~\cite{DBLP:journals/corr/SaxeMG13},
затем, последовательно для каждого слоя настраивает весовые коэффициенты таким образом, чтобы дисперсия
выходных данных слоя была единичной. Эксперименты показали, что такая инициализация позволяет эффективно 
обучать очень глубокие нейронные сети. Далее будут описаны отобранные для конечного ансамбля модели, их архитектуры, 
свойства и особенности обучения.

\subsubsection{Модель I}
Модель I состоит из 18 обучающихся слоёв и $\approx$\,2,7 млн. параметров.
Данная нейронная сеть достигла самой высокой точности предсказания среди всех обученных моделей (93,4\%). 
Maxout в качестве функции активации является её отличительной особенностью.
На рисунке~\ref{fig:model_I_maxout_example} показано как maxout соединён с выходами свёрточных слоёв. На
рисунке~\ref{fig:model_I_maxout_ip_example} можно видеть соединение выходов полносвязного слоя и активации.

\begin{figure}[H]
\centering
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=\textwidth]{model_I_maxout_example}
  \captionof{figure}{Соединение maxout со\\
                     свёрточными слоями}
  \label{fig:model_I_maxout_example}
\end{minipage}%
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=\textwidth]{model_I_maxout_ip_example}
  \captionof{figure}{Соединение maxout с\\
                     полносвязными слоями}
  \label{fig:model_I_maxout_ip_example}
  \vspace*{1.4cm}
\end{minipage}
\vspace*{-1.4cm}
\end{figure}

Нейронная сеть обучалась 90\,000 итераций методом стохастического градиентного спуска,
с начальным learning rate (lr) 0,01, моментом 0.9, L2 регуляризацией с коэффициентом 0,0005 и 
мини батчем (mini batch) из 256 изображений. В процессе обучения, начиная с 40\,000 итерации, 
lr уменьшался в десять раз каждые 20\,000 итераций, достигнув к концу обучения значения $10^{-5}$.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{confusion_matrix_model_I}
    \vspace*{-1cm}
    \caption{Матрица предсказаний модели I}
    \label{fig:confusion_matrix_model_I}
\end{figure}

\subsubsection{Модели II и III}
Модель II имеет в 4,5 раза меньше параметров, в сравнении с моделью I ($\approx$\,600\,000).
Данная нейронная сеть имеет классическую архитектуру --- чередование свёрточных и pooling слоёв, в качестве
функции активации используется Leaky ReLU\footnote{$f(x) = \max(x, \alpha x)$} с параметром $\alpha = 0,01$.

Сеть обучалась в течении 30\,000 итераций. Функционал ошибки оптимизировался алгоритмом NAG (Nesterov accelerated gradient)
\cite{SutskeverMartensDahlHinton_icml2013}, с начальным learning rate (lr) 0,01, моментом 0.9, L2 регуляризацией с коэффициентом 0,0005 и 
мини батчем из 256 изображений. Начиная с 15\,000 итерации, 
lr уменьшался в десять раз каждые 5\,000 итераций, достигнув к концу обучения значения $10^{-5}$.

Результат модели II оказался ниже на тестовом множестве, в сравнении с моделью I (89,48\% против 93,4\%).
Данный результат объясняется недообучением, что является следствием недостаточного числа параметров в нейронной сети.
С другой стороны, за счёт уменьшения числа весов в модели, удалось снизить время обучения в 10 раз (с десяти часов до одного).

Модель III является попыткой исправить главный недостаток модели II за счёт увеличения числа параметров.
В связи с чем, число фильтров каждого свёрточного слоя было увеличено в два раза, что позволило снизить ошибку на 1,55\%.
Параметры оптимизации использовались такие же как в модели II.

\begin{figure}[H]
\centering
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=\textwidth]{confusion_matrix_model_II}
  \vspace*{-1.7cm}
  \captionof{figure}{Матрица предсказаний\\модели II}
\end{minipage}%
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=\textwidth]{confusion_matrix_model_III}
  \vspace*{-1.7cm}
  \captionof{figure}{Матрица предсказаний\\модели III}
\end{minipage}
\end{figure}

\subsubsection{Модели IV и V}
На моделях IV и V проверялась гипотеза, что pooling слои могут быть заменены на соответствующие свёрточные слои
с увеличенным параметром stride без значительного снижения точности распознавания. \cite{DBLP:journals/corr/SpringenbergDBR14}.
В данных сетях max-pooling~2$\times$2 слои были заменены  на свёрточные слои со stride равным двум.

Модель IV имеет $\approx$\,6 млн. параметров и 13 слоёв. В качестве функции активации используется ReLU.
Параметры оптимизации использовались такие же как и в моделях II и III, за исключением увеличенного
до 50\,0000 числа итераций SGD. После обучения в течении 14 часов, точность на тестовом множестве достигла 92,1\%, что
подтверждает гипотезу о замене pooling слоёв.

Модель V состоит из 16 слоёв и $\approx$\,7,8 млн свободных параметров. Данная нейронная сеть является попыткой
улучшить точность модели IV за счёт увеличения числа свёрточных слоёв. Однако, дополнительные слои привнесли
дополнительно $\approx$\,1,8 млн. параметров, что привело к переобучению и снижении конечной точности на 1,19\%.
Модель V обучалась в течении 19 часов, с мини батчем из 128 изображений и используя такие же оптимизационные параметры
как и модель IV.

\begin{figure}[H]
\centering
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=\textwidth]{confusion_matrix_model_IV}
  \vspace*{-1.7cm}
  \captionof{figure}{Матрица предсказаний\\модели IV}
\end{minipage}%
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=\textwidth]{confusion_matrix_model_V}
  \vspace*{-1.7cm}
  \captionof{figure}{Матрица предсказаний\\модели V}
\end{minipage}
\end{figure}

\subsubsection{Модель VI}
Модель VI состоит из 18 слоёв и $\approx$\,4,2 млн. параметров. Данная модель использует слои batch normalization, 
которые преобразовывают активации свёрточных и полносвязных слоев таким образом, что их среднее равно нулю, а
дисперсия единице (значения вычисляются на всём mini batch). Авторы batch normalization \cite{DBLP:journals/corr/IoffeS15}
утверждают, что данный приём позволяет использовать высокий learning rate и приводит к более быстрой сходимости.
Данная нейронная сеть обучалась 70\,000 итераций с помощью SGD. Модель VI достигла точности в 92,0\% за 25\,0000 итераций, 
обучение сети заняло 10 часов.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{batchnrm_net_VI}
    \caption{Batch normalization слой в модели~VI}
    \label{fig:model_VI_bnrm}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{confusion_matrix_model_VI}
    \vspace*{-1cm}
    \caption{Матрица предсказаний модели VI}
    \label{fig:confusion_matrix_model_VI}
\end{figure}

\subsubsection{Модель VII}
Модель VII идентична модели I, за исключением количества свёрточных слоёв соединённых с maxout активацией. В данной модели их пять
(Рис. \ref{fig:model_VII_maxout}), тогда как модель I имеет только два слоя. Такое изменение в топологии сети не привело к 
увеличению конечной точности модели, более того, ошибка увеличилась на 0,9\%. Максимальное число итераций достигло 70\,000,
время затраченное на обучение составило 22 часа.

\begin{figure}[H]
\centering
\begin{minipage}{.5\textwidth}
  \centering
  \vspace*{0.7cm}
  \includegraphics[width=\textwidth]{model_VII_maxout}
  \vspace*{-0.4cm}
  \captionof{figure}{Соединение maxout \\активаций в модели~VII}
  \label{fig:model_VII_maxout}
\end{minipage}%
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=\textwidth]{confusion_matrix_model_VII}
  \vspace*{-1.7cm}
  \captionof{figure}{Матрица предсказаний\\модели VII}
  \label{fig:confusion_matrix_model_VII}
\end{minipage}
\end{figure}

\subsection{Объединение нейронных сетей в ансамбль}
Объединение нескольких моделей в ансамбль является мощным приёмом при решении различных задач машинного обучения.
Применение подобной техники было выбрано по причине наличия различных свёрточных нейронных сетей, предсказания которых
не очень сильно коррелируют между собой. В данном исследовании создание ансамбля производилось при помощи <<стэкинга>> (stacking)
\cite{Wolpert92stackedgeneralization}, идея которого заключается в построении алгоритма, объединяющего
предсказания базовых моделей (в данном случае нейронных сетей). Такой подход зачастую работает лучше, 
так как усиливает обобщающую способность и робастность конечного классификатора за счёт одиночных моделей.

Для построения конечного ансамбля были протестированы различные алгоритмы классификации, такие как случайный лес,
метод k ближайших соседей, машина опорных векторов и Extra Trees. Наилучшая точность была достигнута с 
помощью алгоритма Extra Trees,\cite{ExtraTrees} (94,21\%), который сам является ансамблем 
деревьев решений. Схема объединения моделей отражена на рисунке \ref{fig:ansamble}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{ensamble}
    \caption{Схема конечного ансамбля}
    \label{fig:ansamble}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{ensamble_confusion_matrix}
    \vspace*{-1.3cm}
    \caption{Матрица предсказаний ансамбля моделей}
    \label{fig:ensamble_confusion_matrix}
\end{figure}